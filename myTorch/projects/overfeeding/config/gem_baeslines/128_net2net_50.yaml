config_filename: "default.yaml"
description: "Model to evaluate the performance in presence of the GEM model"

project_name: "GEMAnalysis"
ex_name: "128_net2net_50"


# model specific details

model:  "LSTM"
input_size: 8
output_size:  8
num_layers: 1
layer_size: [128]
activation: "tanh"

# optimization specific details

# Valid optimizer names: Adadelta, Adagrad, Adam, RMSprop, SGD
optim_name: "Adam"
lr: 1.0e-3
rho: 0.9
eps:  1.0e-8
weight_decay: 0.0
lr_decay: 0.0
beta_0: 0.9
beta_1: 0.999
alpha:  0.99
momentum: 0.0
centered: False
dampening:  0.0
nesterov: False
grad_clip:  [-10.0, 10.0]

max_steps: 50000
rseed:  5
device: "cuda:0" # can be cpu or cuda or cuda:1, cuda:2

# task specific details
task: "copy"
num_bits: 8
seq_len: 5
batch_size: 10

# curriculum related details
min_seq_len: 5
max_seq_len: 21
step_seq_len: 5
average_over_last_n: 50
evaluate_over_n: 50
curriculum_seed: 6
new_lr: 0.001

# early stopping related details
# Now it is used to determine when the model should be grown.
#time_span: 10000

# Details related to expanding the rnn
expand_model: True
# This is the amount by which we expand the model
expansion_offset: 50
expanded_layer_size: [40]
make_optimizer_wider: False
expand_model_weights: True
use_noise: True
use_random_noise: True

# saving details
use_tflogger: True
save_every_n: 1000000000000
log_grad_norm: True

# GEM details
use_gem: False
memory_strength: 0.5
num_memories: 1
use_regularisation: False
regularisation_constant: 0.0
use_projection: False