config_filename: "default.yaml"
description: "Models for AAAI"

project_name: "AAAI/copy"
ex_name: "256"


# model specific details

model:  "LSTM"
input_size: 8
output_size:  8
num_layers: 1
layer_size: [256]
activation: "tanh"

# optimization specific details

# Valid optimizer names: Adadelta, Adagrad, Adam, RMSprop, SGD
optim_name: "Adam"
lr: 1.0e-3
rho: 0.9
eps:  1.0e-8
weight_decay: 0.0
lr_decay: 0.0
beta_0: 0.9
beta_1: 0.999
alpha:  0.99
momentum: 0.0
centered: False
dampening:  0.0
nesterov: False
grad_clip:  [-10.0, 10.0]

max_steps: 20000
rseed:  5
device: "cuda" # can be cpu or cuda or cuda:1, cuda:2

# task specific details
task: "copy"
num_bits: 8
seq_len: 5
batch_size: 10

# curriculum related details
min_seq_len: 5
max_seq_len: 63
step_seq_len: 3
average_over_last_n: 100
evaluate_over_n: 100
curriculum_seed: 6
new_lr: 0.001
stop_if_curriculum_failed: True
threshold_accuracy_for_passing_curriculum: 0.75


# early stopping related details
# Now it is used to determine when the model should be grown.
#wait_time_before_expanding: 10000

# Details related to expanding the rnn
expand_model: False
# This is the amount by which we expand the model
expansion_offset: -1
expanded_layer_size: [80]
make_optimizer_wider: False
expand_model_weights: True
use_noise: True
use_random_noise: True

# saving details
use_tflogger: True
save_every_n: 1000000000000
log_grad_norm: False

# GEM details
use_gem: False
memory_strength: 0.5
num_memories: 1
use_regularisation: False
regularisation_constant: 0.0
use_projection: False
add_gradients: False
normalise_gradient_before_adding: False