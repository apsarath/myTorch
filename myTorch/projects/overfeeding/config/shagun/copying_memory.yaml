parent_config: "config/default.yaml"
config_filename: "copying_memory.yaml"
description: "widen_lstm_copy_task"

project_name: "copying_memory_task"
ex_name: "ex05"

# model specific details

model:  "LSTM"
input_size: 8
output_size:  8
num_layers: 1
layer_size: [128]
activation: "tanh"

# optimization specific details

# Valid optimizer names: Adadelta, Adagrad, Adam, RMSprop, SGD
optim_name: "SGD"
lr: 1.0e-3
rho: 0.9
eps:  1.0e-8
weight_decay: 0.0
lr_decay: 0.0
beta_0: 0.9
beta_1: 0.999
alpha:  0.99
momentum: 0.0
centered: False
dampening:  0.0
nesterov: False
grad_clip:  [-10.0, 10.0]

max_steps:  100000000
rseed:  5
device: "cuda" # can be cpu or cuda or cuda:1, cuda:2

# task specific details
task: "copy"
num_bits: 8
seq_len: 5
batch_size: 10

# curriculum related details
min_seq_len:  5
max_seq_len:  101
step_seq_len: 3
average_over_last_n: 200
evaluate_over_n: 1000
curriculum_seed: 6

# early stopping related details
time_span: 5000

# Details related to expanding the rnn
expanded_layer_size: [256]
make_optimizer_wider: True

# saving details
use_tflogger: True
save_every_n: 1000000000000