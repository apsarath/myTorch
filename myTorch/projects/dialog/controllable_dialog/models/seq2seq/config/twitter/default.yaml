config_filename: "default.yaml"
description: "seq2seq twitter"
project_name: "seq2seq_twitter"
ex_name: "seq2seq_3layer_gru_512"
base_data_path: "/mnt/data/chinna/data/datasets/seq2seq/twitter/data/"
dataset: "twitter"
num_dialogs: 1000000 # valid ones are 100, 1000, 200000, 500000, 10000000

# Model details
emb_size_src: 300
hidden_dim_src: 512
hidden_dim_tgt: 512
bidirectional: False
nlayers_src: 2
nlayers_tgt: 2
dropout_rate: 0.3

# Dataset specific details
eou: "<eou>"
go: "<go>"
unk: "<unk>"
num: "<num>"
pad: "<pad>"
sent_cut_off: 30
min_sent_len: 7
train_valid_split: 0.95
vocab_cut_off: 25000
embedding_loc: "None"
# "/mnt/data/chinna/data/glove/glove.6B.300d.txt"

# Valid optimizer names: Adadelta, Adagrad, Adam, RMSprop, SGD  
optim_name: "Adam"
lr: 1.0e-3
rho: 0.9
eps:  1.0e-8
weight_decay: 0.0
lr_decay: 0.0
beta_0: 0.9                                                                                                                   
beta_1: 0.999
alpha:  0.99
momentum: 0.0
centered: False
dampening:  0.0
nesterov: False
grad_clip_norm: 5.0

# task specific details
device: "cuda"
num_epochs: 200
batch_size: 20
task: "twitter_lm"
eval_batch_size: 32
test_batch_size: 32
rseed: 5
time_lag: 100
seq_len: 10
bptt: 70

# saving details
use_tflogger: True
save_every_n: 500
inter_saving: [0, 1000, 2000, 3000, 4000, 5000]
