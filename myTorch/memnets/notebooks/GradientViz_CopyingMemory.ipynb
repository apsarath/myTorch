{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from myTorch.task.copying_memory import CopyingMemoryData\n",
    "from myTorch.memnets.load import load_experiment\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list()\n",
    "model_names = list()\n",
    "model_colors = list()\n",
    "models.append(\"/mnt/data/sarath/mytorch_savedir/output/copying_memory_task/ex01/current\")\n",
    "model_names.append(\"RNN\")\n",
    "model_colors.append(\"r\")\n",
    "models.append(\"/mnt/data/chinna/output/copying_memory_task/mask_1/current\")\n",
    "model_names.append(\"LSTM\")\n",
    "model_colors.append(\"b\")\n",
    "models.append(\"/mnt/data/chinna/output/copying_memory_task/flatmemory_m64_k4_h80_linear/current\")\n",
    "model_names.append(\"FlatMemory\")\n",
    "model_colors.append(\"g\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(save_dir):\n",
    "    experiment, model, data_iterator, device, config = load_experiment(save_dir)\n",
    "    config.time_lag = 100\n",
    "    config.batch_size = 1\n",
    "    config.seed = 1000\n",
    "    new_data_iterator = CopyingMemoryData(seq_len=config.seq_len, time_lag=config.time_lag,\n",
    "                                          batch_size=config.batch_size, seed=config.seed)\n",
    "    data_iterator = new_data_iterator\n",
    "    \n",
    "    data = data_iterator.next()\n",
    "    seqloss = 0\n",
    "\n",
    "    model.reset_hidden(batch_size=1)\n",
    "\n",
    "    hiddens = []\n",
    "\n",
    "    for i in range(0, data[\"datalen\"]):\n",
    "\n",
    "        x = torch.from_numpy(numpy.asarray(data['x'][i])).to(device)\n",
    "        y = torch.from_numpy(numpy.asarray(data['y'][i])).to(device)\n",
    "        mask = float(data[\"mask\"][i])\n",
    "\n",
    "        model.optimizer.zero_grad()\n",
    "\n",
    "        output = model(x)\n",
    "        hiddens.append(model._h_prev[0][\"h\"])\n",
    "        hiddens[-1].requires_grad_()\n",
    "        if config.task == \"copying_memory\":\n",
    "            loss = F.torch.nn.functional.cross_entropy(output, y.squeeze(1))\n",
    "        else:\n",
    "            loss = F.binary_cross_entropy_with_logits(output, y)\n",
    "\n",
    "        seqloss += (loss * mask)\n",
    "\n",
    "    seqloss /= sum(data[\"mask\"])\n",
    "\n",
    "    hidden_norms = list()\n",
    "    hidden_diff = list()\n",
    "    hidden_grads = list()\n",
    "    for i in range(0, len(hiddens)):\n",
    "        hidden_diff.append((hiddens[i]-hiddens[-1]).norm().item())\n",
    "        hidden_norms.append(hiddens[i].norm().item())\n",
    "    for i in range(0, len(hiddens)):\n",
    "        model.optimizer.zero_grad()\n",
    "        grad = torch.autograd.grad(loss, hiddens[i], retain_graph=True)\n",
    "        hidden_grads.append(grad[0].norm().item())\n",
    "    return hidden_norms, hidden_diff, hidden_grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using cpu\n",
      "/mnt/data/sarath/code/myTorch/myTorch/memory/RNNCell.py:80: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(self._W_i2h, gain=nn.init.calculate_gain(self._activation))\n",
      "/mnt/data/sarath/code/myTorch/myTorch/memnets/recurrent_net.py:84: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(self._W_h2o, gain=nn.init.calculate_gain(self._output_activation))\n",
      "/mnt/data/sarath/code/myTorch/myTorch/memnets/recurrent_net.py:85: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(self._b_o, 0)\n",
      "INFO:root:Loading the experiment from /mnt/data/sarath/mytorch_savedir/output/copying_memory_task/ex01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_params : 8009 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using cpu\n",
      "INFO:root:Loading the experiment from /mnt/data/chinna/output/copying_memory_task/mask_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_params : 8649 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using cpu\n",
      "INFO:root:Loading the experiment from /mnt/data/chinna/output/copying_memory_task/flatmemory_m64_k4_h80_linear\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_params : 23569 \n"
     ]
    }
   ],
   "source": [
    "hidden_norm_list = list()\n",
    "hidden_diff_list = list()\n",
    "hidden_grad_list = list()\n",
    "\n",
    "for i in range(0, len(models)):\n",
    "    hidden_norm, hidden_diff, hidden_grad = get_stats(models[i])\n",
    "    hidden_norm_list.append(hidden_norm)\n",
    "    hidden_diff_list.append(hidden_diff)\n",
    "    hidden_grad_list.append(hidden_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(models)):\n",
    "    plt.plot(hidden_norm_list[i], color=model_colors[i], label=model_names[i])\n",
    "\n",
    "plt.ylabel('hidden state norm')\n",
    "plt.xlabel('Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for i in range(0, len(models)):\n",
    "    plt.plot(hidden_diff_list[i], color=model_colors[i], label=model_names[i])\n",
    "\n",
    "plt.ylabel('distance to last hidden state')\n",
    "plt.xlabel('Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for i in range(0, len(models)):\n",
    "    plt.plot(hidden_grad_list[i], color=model_colors[i], label=model_names[i])\n",
    "\n",
    "plt.ylabel('Gradient norms')\n",
    "plt.xlabel('Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
